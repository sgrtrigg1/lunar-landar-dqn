{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964ab60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from tqdm import trange\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4600d",
   "metadata": {},
   "source": [
    "üéí Step 1: Replay Buffer Code Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19342765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéí Replay Buffer - Experience Replay Memory\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A fixed-size buffer that stores agent experiences for training the Q-network.\n",
    "\n",
    "    Attributes:\n",
    "        buffer (deque): A double-ended queue to store experience tuples.\n",
    "        capacity (int): Maximum number of experiences the buffer can hold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=100_000):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): Maximum number of transitions to store in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a new experience in the buffer.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (np.ndarray): The resulting next state.\n",
    "            done (bool): Whether the episode has ended.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The number of samples to return.\n",
    "\n",
    "        Returns:\n",
    "            tuple of np.ndarrays: Batched (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.uint8)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of stored experiences.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53fd59",
   "metadata": {},
   "source": [
    "üß† Step 2: Q-Network (Neural Net Brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538c04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Q-Network - The Deep Neural Network Approximator\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network that estimates Q-values for each action\n",
    "    given a state input from the environment.\n",
    "\n",
    "    Attributes:\n",
    "        model (nn.Sequential): The sequential network architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Initialize the Q-network layers.\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of the input state.\n",
    "            action_size (int): Number of possible discrete actions.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input state tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The predicted Q-values for each action.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdf876",
   "metadata": {},
   "source": [
    "ü§ñ Step 3: DQN Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ DQN Agent - The Reinforcement Learning Engine\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Learning Agent that interacts with and learns from the environment.\n",
    "\n",
    "    Attributes:\n",
    "        q_network (QNetwork): Main Q-network used to predict Q-values.\n",
    "        target_network (QNetwork): Target Q-network for stable learning.\n",
    "        optimizer (torch.optim.Adam): Optimizer for updating q_network weights.\n",
    "        buffer (ReplayBuffer): Memory buffer for storing experiences.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        epsilon (float): Exploration rate for epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, buffer, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Number of features in the environment state.\n",
    "            action_size (int): Number of discrete actions.\n",
    "            buffer (ReplayBuffer): Replay buffer instance.\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "            gamma (float): Discount factor.\n",
    "            epsilon (float): Initial exploration rate.\n",
    "            epsilon_decay (float): Rate at which epsilon decays.\n",
    "            epsilon_min (float): Minimum value of epsilon.\n",
    "        \"\"\"\n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.target_network = QNetwork(state_size, action_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = buffer\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): Current state.\n",
    "\n",
    "        Returns:\n",
    "            int: Action index.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch from memory and perform a learning step.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Size of the mini-batch for learning.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the exploration rate after each episode.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Copy weights from the main Q-network to the target network.\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958fd44",
   "metadata": {},
   "source": [
    "üîÅ Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Training Loop - Interact, Learn, and Track Progress\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Agent setup\n",
    "buffer = ReplayBuffer(capacity=100_000)\n",
    "agent = DQNAgent(state_size, action_size, buffer)\n",
    "\n",
    "# Training hyperparameters\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "target_update_freq = 10  # how often to sync target network\n",
    "\n",
    "# Tracking\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for episode in trange(num_episodes, desc=\"üöÄ Training\"):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    losses = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Learning step\n",
    "        loss = agent.learn(batch_size)\n",
    "        if loss is not None:\n",
    "            losses.append(loss)\n",
    "\n",
    "    # End-of-episode updates\n",
    "    agent.update_epsilon()\n",
    "    if episode % target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Track performance\n",
    "    reward_history.append(total_reward)\n",
    "    avg_loss = np.mean(losses) if losses else 0\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    # Print episode summary every 100\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"üéØ Episode {episode+1} - Reward: {total_reward:.2f} | Epsilon: {agent.epsilon:.3f} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17503418",
   "metadata": {},
   "source": [
    "üíæ Save the Trained Model (Run This Once Training is Done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Q-network if already trained\n",
    "\n",
    "model_path = \"results/models/dqn_agent.pth\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(agent.q_network.state_dict(), model_path)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af974ba",
   "metadata": {},
   "source": [
    "üìà Step 5: Plot Rewards and Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a264e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Plotting Reward and Training Loss Over Time\n",
    "\n",
    "# Plot total reward per episode\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(reward_history, label=\"Episode Reward\")\n",
    "plt.title(\"üöÄ Total Reward per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"results/plots/reward_curve.png\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history, label=\"Training Loss\", color=\"orange\")\n",
    "plt.title(\"üìâ Average Loss per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"results/plots/loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676b753",
   "metadata": {},
   "source": [
    "üé• Step 6: Record a Video of the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé• Record the Trained Agent Landing\n",
    "\n",
    "# Define video output folder\n",
    "video_dir = \"results/videos\"\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Create a wrapped environment for recording\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_dir,\n",
    "    episode_trigger=lambda episode_id: True,\n",
    "    name_prefix=\"lunar_lander\"\n",
    ")\n",
    "\n",
    "# Reload best-performing agent (optional if already in memory)\n",
    "agent.epsilon = 0.0  # Turn off exploration for evaluation\n",
    "\n",
    "for episode in range(3):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"üé¨ Episode {episode + 1} finished with reward: {total_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cfb7e",
   "metadata": {},
   "source": [
    "üéûÔ∏è Convert .mp4 to .gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåÄ Convert Recorded MP4 to GIF\n",
    "\n",
    "# Locate your video\n",
    "video_path = \"results/videos/lunar_lander-episode-0.mp4\"\n",
    "gif_path = \"results/videos/lunar_lander.gif\"\n",
    "\n",
    "# Read frames and save as GIF\n",
    "reader = imageio.get_reader(video_path, format=\"ffmpeg\")\n",
    "fps = reader.get_meta_data()[\"fps\"]\n",
    "\n",
    "writer = imageio.get_writer(gif_path, format=\"GIF\", fps=fps)\n",
    "\n",
    "for frame in reader:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "print(f\"‚úÖ GIF saved at: {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5cc91",
   "metadata": {},
   "source": [
    "üéØ 1. Evaluate and Capture the Best Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Record Only the Best Landing Episode for GIF\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "best_reward = -float(\"inf\")\n",
    "best_frames = []\n",
    "\n",
    "agent.epsilon = 0.0  # No exploration during evaluation\n",
    "\n",
    "for episode in range(5):  # Run 5 evaluation episodes\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        frame = env.render()  # Capture current frame\n",
    "        frames.append(frame)\n",
    "\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"üé¨ Episode {episode+1} reward: {total_reward:.2f}\")\n",
    "\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        best_frames = frames.copy()\n",
    "\n",
    "env.close()\n",
    "print(f\"üèÜ Best reward: {best_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938c546",
   "metadata": {},
   "source": [
    "üåÄ 2. Save the Best Frames as a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Best Landing as a GIF\n",
    "\n",
    "os.makedirs(\"results/videos\", exist_ok=True)\n",
    "gif_path = \"results/videos/best_landing.gif\"\n",
    "\n",
    "imageio.mimsave(gif_path, best_frames, fps=30)\n",
    "print(f\"‚úÖ GIF saved to {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3567c9",
   "metadata": {},
   "source": [
    "üì¶ Load the Trained Model Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Load a previously trained Q-network\n",
    "\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "buffer = ReplayBuffer()\n",
    "agent = DQNAgent(state_size, action_size, buffer)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path = \"results/models/dqn_agent.pth\"\n",
    "agent.q_network.load_state_dict(torch.load(model_path))\n",
    "agent.q_network.eval()  # Set to evaluation mode\n",
    "agent.epsilon = 0.0     # Turn off exploration\n",
    "\n",
    "print(\"‚úÖ Loaded trained model and ready for evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
